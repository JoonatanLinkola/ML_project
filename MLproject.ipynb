{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c04fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # data visualization library\n",
    "import zipfile as zf  # used for extracting the dataset from the zip file\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix  # evaluation metrics\n",
    "\n",
    "from sklearn.datasets import fetch_openml \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# image representation \n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "\n",
    "# for calculating the size of the dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601730cd",
   "metadata": {},
   "source": [
    "Before continuing forward, unzip the dataset zip-file to the directory of the project (unzipping from the terminal suggested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" # the character set in read order\n",
    "\n",
    "char_labels = [] # characters to more usable form, i.e. [\"A\", \"B\", ... , \"Z\"]\n",
    "for i in range (0, len(charset)):\n",
    "    char_labels.append(charset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebd7d2",
   "metadata": {},
   "source": [
    "After running the cells above, feel free to either continue with the PCA model or jump straight to the CNN model without running the PCA analysis code cells (some of the methods can take some time to execute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dcc52b",
   "metadata": {},
   "source": [
    "# Method 1: PCA (principal copmponent analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "total_count = 0 # count the number of files altogether\n",
    "\n",
    "for folder in range(33, 58+1): # iterate through all character folders 33..58\n",
    "    \n",
    "    dir_path = f'character_set_A_to_Z/{folder}'\n",
    "    file_count = len([entry for entry in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, entry))])\n",
    "    total_count = total_count + file_count\n",
    "    \n",
    "    for n in range(file_count): # iterate through each character images in each file\n",
    "        img = Image.open(f\"character_set_A_to_Z/{folder}/{char_labels[folder-33]} ({n+1}).png\")\n",
    "        data = asarray(img)\n",
    "        features.append(data)\n",
    "        labels.append(f\"{char_labels[folder-33]}\")\n",
    "        \n",
    "X = np.array(features).reshape(total_count, 64*64) # \"flatten\" the matrix representation to numpy array form\n",
    "y = np.array(labels)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765cc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split feature and label data into 80/20 split for PCA analysis\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.22, random_state=1) \n",
    "# test size is 0.22 = 22%\n",
    "    \n",
    "print(X_train.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(solver='liblinear' )\n",
    "clf.fit(X_train[:X_train.shape[0]], y_train[:y_train.shape[0]]) # let's train with the whole training set\n",
    "# can take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_conf_mat(y_true, y_pred): # function for visualiaing the confusion matrix\n",
    "    ax = plt.subplot()\n",
    "    conf_mat = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d', ax=ax, xticklabels=char_labels, yticklabels=char_labels, cmap=\"mako\")\n",
    "\n",
    "    ax.set_xlabel('Predicted labels', fontsize=20)\n",
    "    ax.set_ylabel('True labels', fontsize=20)\n",
    "    ax.set_title('Confusion Matrix', fontsize=20)\n",
    "\n",
    "y_pred = clf.predict(X_val)\n",
    "gen_conf_mat(y_val, y_pred)\n",
    "plt.gcf().set_size_inches(15,13)\n",
    "plt.show()\n",
    "\n",
    "multi_accuracy = accuracy_score(y_val, y_pred) # compute the accuracy\n",
    "print(f\"Prediction accuracy: {100*multi_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4db5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the PCA\n",
    "N = 50\n",
    "pca = PCA(n_components=N)\n",
    "X_train_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6097177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the explained variances\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "color = 'darkgreen'\n",
    "ax1.bar(1+np.arange(N), pca.explained_variance_ratio_, color=color)\n",
    "ax1.set_xticks(10+np.arange(N, step=10))\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylabel(\"Explained variance ratio\", color=color)\n",
    "ax1.set_xlabel(\"Generated feature\")\n",
    "plt.grid(linestyle='--')\n",
    "\n",
    "# plot the cumulative variances\n",
    "color = 'navy'\n",
    "ax2 = ax1.twinx()\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.plot(1+np.arange(N), np.cumsum(pca.explained_variance_ratio_), color=color)\n",
    "ax2.set_ylabel(\"Cumulative explained variance ratio\", color=color)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ae732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [5, 8, 10, 20] # Prediction accuracy with three different values for N\n",
    "# can take some time\n",
    "\n",
    "for each in N:\n",
    "    pca.set_params(n_components=each)\n",
    "    X_train_reduced = pca.fit_transform(X_train)\n",
    "    \n",
    "    clf_2 = LogisticRegression(solver='sag') # as above, but with different values of N\n",
    "    clf_2.fit(X_train_reduced, y_train)\n",
    "    X_val_reduced = pca.fit_transform(X_val)\n",
    "    y_pred = clf_2.predict(X_val_reduced) # compute the prediction on the validating set\n",
    "    multi_accuracy = accuracy_score(y_val, y_pred) # compute the accuracy score\n",
    "    \n",
    "    # gen_conf_mat(y_val, y_pred)\n",
    "    # plt.gcf().set_size_inches(15,13)\n",
    "    # plt.show()\n",
    "\n",
    "    print(f\"Prediction accuracy with N = {each}: {100*multi_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca545af3",
   "metadata": {},
   "source": [
    "# Method 2: CNN (convolutional neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dbb341",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "total_count = 0 # count the number of files altogether\n",
    "\n",
    "for folder in range(33, 58+1): # iterate through all character folders 33..58\n",
    "    \n",
    "    dir_path = f'character_set_A_to_Z/{folder}'\n",
    "    file_count = len([entry for entry in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, entry))])\n",
    "    total_count = total_count + file_count\n",
    "    \n",
    "    for n in range(file_count): # iterate through each character images in each file\n",
    "        img = Image.open(f\"character_set_A_to_Z/{folder}/{char_labels[folder-33]} ({n+1}).png\")\n",
    "        data = asarray(img)\n",
    "        features.append(data)\n",
    "        labels.append(folder-33)\n",
    "        # this time the labels are numbers from 0 to 25 in the order A = 0, B = 1, etc. \n",
    "        # this is due to how the tensionflow.keras Sequential model below works\n",
    "        \n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1dc1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split feature and label data into 78/22 split for CNN analysis\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=1) \n",
    "# test size is 0.22 = 22%\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 64, 64, 1) # convert to 4D array for later use\n",
    "X_test = X_test.reshape(X_test.shape[0], 64, 64, 1)\n",
    "input_shape = (64, 64, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train = X_train/255 # dividing by the max value for greyscale images\n",
    "X_test = X_test/255\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8df394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential # import the necessary libraries\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential() # building the CNN model\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = (3, 3), input_shape = input_shape))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation = tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(26, activation = tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5842b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(x = X_train, y = y_train, epochs = 10)\n",
    "# the number of epochs, as well as the optimiser and loss function can be changed aroud for different results\n",
    "\n",
    "model.evaluate(X_test, y_test) # evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c10281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
